{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anna.txt','r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Convert characters to intergers and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "char2int = {ch:ii for ii,ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 : C\n",
      "28 : h\n",
      "25 : a\n",
      "32 : p\n",
      "22 : t\n",
      "69 : e\n",
      "49 : r\n",
      "63 :  \n",
      "74 : 1\n",
      "12 : \n",
      "\n",
      "12 : \n",
      "\n",
      "12 : \n",
      "\n",
      "21 : H\n",
      "25 : a\n",
      "32 : p\n",
      "32 : p\n",
      "29 : y\n",
      "63 :  \n",
      "56 : f\n",
      "25 : a\n",
      "30 : m\n",
      "17 : i\n",
      "55 : l\n",
      "17 : i\n",
      "69 : e\n",
      "8 : s\n",
      "63 :  \n",
      "25 : a\n",
      "49 : r\n",
      "69 : e\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print('{} : {}'.format(encoded[i],int2char[encoded[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr,n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]),arr.flatten()] = 1\n",
    "    \n",
    "    one_hot = one_hot.reshape((*arr.shape,n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function\n",
    "\n",
    "one_hot_val = one_hot_encode(np.array([[1,2,3]]),8)\n",
    "\n",
    "one_hot_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr,batch_size,seq_length):\n",
    "    batch_ele_count = batch_size * seq_length\n",
    "    \n",
    "    batches = len(arr) // batch_ele_count\n",
    "    \n",
    "    arr = arr[:(batches*batch_ele_count)]\n",
    "    \n",
    "    arr = arr.reshape(batch_size,-1)\n",
    "    \n",
    "    for n in range(0,arr.shape[1],seq_length):\n",
    "        x = arr[:,n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:,:-1] , y[:,-1] = x[:,1:] , x[:,n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:,:-1] , y[:,-1] = x[:,1:] , x[:,0]\n",
    "            \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = get_batches(np.array([1,2,3,4,5,6,7,8,9,10]),2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x , y = next(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[1 2 3]\n",
      " [4 5 6]] \n",
      "\n",
      " y: [[2 3 1]\n",
      " [5 6 4]]\n"
     ]
    }
   ],
   "source": [
    "print('x: {} \\n\\n y: {}'.format(x[:10,:10] , y[:10,:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU Available\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    print(\"GPU is present\")\n",
    "    gpu = True\n",
    "else:\n",
    "    print(\"No GPU Available\")\n",
    "    gpu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,tokens,n_hidden=256,n_layers=2,drop_prob=0.5,lr=0.001):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch:ii for ii,ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars),n_hidden,n_layers,dropout=drop_prob,batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden,len(self.chars))\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        \n",
    "        r_output , hidden = self.lstm(x,hidden)\n",
    "        \n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        out = out.reshape(-1,self.n_hidden)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out,hidden\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if(gpu):\n",
    "            hidden = (weight.new(self.n_layers,batch_size,self.n_hidden).zero_().cuda()\\\n",
    "                      , weight.new(self.n_layers,batch_size,self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers,batch_size,self.n_hidden).zero_()\\\n",
    "                      , weight.new(self.n_layers,batch_size,self.n_hidden).zero_())\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = CharRNN(chars,n_hidden,n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.3225... Val Loss: 3.2186\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1762... Val Loss: 3.1310\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1606... Val Loss: 3.1244\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1306... Val Loss: 3.1215\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1566... Val Loss: 3.1192\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1307... Val Loss: 3.1178\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1238... Val Loss: 3.1169\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1359... Val Loss: 3.1160\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1346... Val Loss: 3.1142\n",
      "Epoch: 1/20... Step: 100... Loss: 3.1183... Val Loss: 3.1116\n",
      "Epoch: 1/20... Step: 110... Loss: 3.1222... Val Loss: 3.1060\n",
      "Epoch: 1/20... Step: 120... Loss: 3.0968... Val Loss: 3.0973\n",
      "Epoch: 1/20... Step: 130... Loss: 3.0991... Val Loss: 3.0780\n",
      "Epoch: 2/20... Step: 140... Loss: 3.0798... Val Loss: 3.0434\n",
      "Epoch: 2/20... Step: 150... Loss: 3.0398... Val Loss: 3.0465\n",
      "Epoch: 2/20... Step: 160... Loss: 2.9705... Val Loss: 2.9501\n",
      "Epoch: 2/20... Step: 170... Loss: 2.8955... Val Loss: 2.8783\n",
      "Epoch: 2/20... Step: 180... Loss: 2.8532... Val Loss: 2.8125\n",
      "Epoch: 2/20... Step: 190... Loss: 2.7826... Val Loss: 2.7408\n",
      "Epoch: 2/20... Step: 200... Loss: 2.7399... Val Loss: 2.6927\n",
      "Epoch: 2/20... Step: 210... Loss: 2.6933... Val Loss: 2.6419\n",
      "Epoch: 2/20... Step: 220... Loss: 2.6592... Val Loss: 2.6060\n",
      "Epoch: 2/20... Step: 230... Loss: 2.6352... Val Loss: 2.5754\n",
      "Epoch: 2/20... Step: 240... Loss: 2.6246... Val Loss: 2.5521\n",
      "Epoch: 2/20... Step: 250... Loss: 2.5752... Val Loss: 2.5290\n",
      "Epoch: 2/20... Step: 260... Loss: 2.5496... Val Loss: 2.5087\n",
      "Epoch: 2/20... Step: 270... Loss: 2.5443... Val Loss: 2.4939\n",
      "Epoch: 3/20... Step: 280... Loss: 2.5460... Val Loss: 2.4772\n",
      "Epoch: 3/20... Step: 290... Loss: 2.5121... Val Loss: 2.4599\n",
      "Epoch: 3/20... Step: 300... Loss: 2.5059... Val Loss: 2.4462\n",
      "Epoch: 3/20... Step: 310... Loss: 2.5067... Val Loss: 2.4312\n",
      "Epoch: 3/20... Step: 320... Loss: 2.4784... Val Loss: 2.4167\n",
      "Epoch: 3/20... Step: 330... Loss: 2.4454... Val Loss: 2.4011\n",
      "Epoch: 3/20... Step: 340... Loss: 2.4360... Val Loss: 2.3914\n",
      "Epoch: 3/20... Step: 350... Loss: 2.4538... Val Loss: 2.3748\n",
      "Epoch: 3/20... Step: 360... Loss: 2.3952... Val Loss: 2.3656\n",
      "Epoch: 3/20... Step: 370... Loss: 2.4103... Val Loss: 2.3530\n",
      "Epoch: 3/20... Step: 380... Loss: 2.3894... Val Loss: 2.3404\n",
      "Epoch: 3/20... Step: 390... Loss: 2.3716... Val Loss: 2.3311\n",
      "Epoch: 3/20... Step: 400... Loss: 2.3688... Val Loss: 2.3218\n",
      "Epoch: 3/20... Step: 410... Loss: 2.3770... Val Loss: 2.3106\n",
      "Epoch: 4/20... Step: 420... Loss: 2.3501... Val Loss: 2.2980\n",
      "Epoch: 4/20... Step: 430... Loss: 2.3447... Val Loss: 2.2917\n",
      "Epoch: 4/20... Step: 440... Loss: 2.3337... Val Loss: 2.2811\n",
      "Epoch: 4/20... Step: 450... Loss: 2.2896... Val Loss: 2.2699\n",
      "Epoch: 4/20... Step: 460... Loss: 2.3067... Val Loss: 2.2596\n",
      "Epoch: 4/20... Step: 470... Loss: 2.3051... Val Loss: 2.2516\n",
      "Epoch: 4/20... Step: 480... Loss: 2.2927... Val Loss: 2.2418\n",
      "Epoch: 4/20... Step: 490... Loss: 2.2870... Val Loss: 2.2359\n",
      "Epoch: 4/20... Step: 500... Loss: 2.2898... Val Loss: 2.2269\n",
      "Epoch: 4/20... Step: 510... Loss: 2.3038... Val Loss: 2.2145\n",
      "Epoch: 4/20... Step: 520... Loss: 2.3038... Val Loss: 2.2100\n",
      "Epoch: 4/20... Step: 530... Loss: 2.2523... Val Loss: 2.2025\n",
      "Epoch: 4/20... Step: 540... Loss: 2.2226... Val Loss: 2.1912\n",
      "Epoch: 4/20... Step: 550... Loss: 2.2541... Val Loss: 2.1840\n",
      "Epoch: 5/20... Step: 560... Loss: 2.2245... Val Loss: 2.1768\n",
      "Epoch: 5/20... Step: 570... Loss: 2.2263... Val Loss: 2.1707\n",
      "Epoch: 5/20... Step: 580... Loss: 2.2034... Val Loss: 2.1651\n",
      "Epoch: 5/20... Step: 590... Loss: 2.2079... Val Loss: 2.1555\n",
      "Epoch: 5/20... Step: 600... Loss: 2.1839... Val Loss: 2.1477\n",
      "Epoch: 5/20... Step: 610... Loss: 2.1966... Val Loss: 2.1393\n",
      "Epoch: 5/20... Step: 620... Loss: 2.1720... Val Loss: 2.1384\n",
      "Epoch: 5/20... Step: 630... Loss: 2.2120... Val Loss: 2.1253\n",
      "Epoch: 5/20... Step: 640... Loss: 2.1771... Val Loss: 2.1156\n",
      "Epoch: 5/20... Step: 650... Loss: 2.1679... Val Loss: 2.1094\n",
      "Epoch: 5/20... Step: 660... Loss: 2.1250... Val Loss: 2.1025\n",
      "Epoch: 5/20... Step: 670... Loss: 2.1746... Val Loss: 2.0976\n",
      "Epoch: 5/20... Step: 680... Loss: 2.1656... Val Loss: 2.0899\n",
      "Epoch: 5/20... Step: 690... Loss: 2.1323... Val Loss: 2.0850\n",
      "Epoch: 6/20... Step: 700... Loss: 2.1236... Val Loss: 2.0778\n",
      "Epoch: 6/20... Step: 710... Loss: 2.1328... Val Loss: 2.0733\n",
      "Epoch: 6/20... Step: 720... Loss: 2.1198... Val Loss: 2.0681\n",
      "Epoch: 6/20... Step: 730... Loss: 2.1215... Val Loss: 2.0607\n",
      "Epoch: 6/20... Step: 740... Loss: 2.1088... Val Loss: 2.0516\n",
      "Epoch: 6/20... Step: 750... Loss: 2.0723... Val Loss: 2.0472\n",
      "Epoch: 6/20... Step: 760... Loss: 2.1148... Val Loss: 2.0450\n",
      "Epoch: 6/20... Step: 770... Loss: 2.0986... Val Loss: 2.0382\n",
      "Epoch: 6/20... Step: 780... Loss: 2.0751... Val Loss: 2.0292\n",
      "Epoch: 6/20... Step: 790... Loss: 2.0805... Val Loss: 2.0230\n",
      "Epoch: 6/20... Step: 800... Loss: 2.0691... Val Loss: 2.0242\n",
      "Epoch: 6/20... Step: 810... Loss: 2.0738... Val Loss: 2.0123\n",
      "Epoch: 6/20... Step: 820... Loss: 2.0582... Val Loss: 2.0095\n",
      "Epoch: 6/20... Step: 830... Loss: 2.0915... Val Loss: 2.0040\n",
      "Epoch: 7/20... Step: 840... Loss: 2.0292... Val Loss: 1.9985\n",
      "Epoch: 7/20... Step: 850... Loss: 2.0423... Val Loss: 1.9918\n",
      "Epoch: 7/20... Step: 860... Loss: 2.0453... Val Loss: 1.9855\n",
      "Epoch: 7/20... Step: 870... Loss: 2.0480... Val Loss: 1.9827\n",
      "Epoch: 7/20... Step: 880... Loss: 2.0401... Val Loss: 1.9744\n",
      "Epoch: 7/20... Step: 890... Loss: 2.0298... Val Loss: 1.9723\n",
      "Epoch: 7/20... Step: 900... Loss: 2.0075... Val Loss: 1.9672\n",
      "Epoch: 7/20... Step: 910... Loss: 2.0059... Val Loss: 1.9627\n",
      "Epoch: 7/20... Step: 920... Loss: 2.0064... Val Loss: 1.9546\n",
      "Epoch: 7/20... Step: 930... Loss: 2.0035... Val Loss: 1.9527\n",
      "Epoch: 7/20... Step: 940... Loss: 1.9985... Val Loss: 1.9460\n",
      "Epoch: 7/20... Step: 950... Loss: 2.0150... Val Loss: 1.9415\n",
      "Epoch: 7/20... Step: 960... Loss: 1.9992... Val Loss: 1.9373\n",
      "Epoch: 7/20... Step: 970... Loss: 2.0146... Val Loss: 1.9357\n",
      "Epoch: 8/20... Step: 980... Loss: 1.9894... Val Loss: 1.9254\n",
      "Epoch: 8/20... Step: 990... Loss: 1.9849... Val Loss: 1.9237\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.9673... Val Loss: 1.9181\n",
      "Epoch: 8/20... Step: 1010... Loss: 2.0009... Val Loss: 1.9190\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.9752... Val Loss: 1.9099\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.9609... Val Loss: 1.9087\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.9615... Val Loss: 1.9019\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.9635... Val Loss: 1.8957\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.9492... Val Loss: 1.8916\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.9613... Val Loss: 1.8886\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.9508... Val Loss: 1.8838\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.9393... Val Loss: 1.8805\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.9361... Val Loss: 1.8774\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.9366... Val Loss: 1.8741\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.9402... Val Loss: 1.8669\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.9227... Val Loss: 1.8659\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.9432... Val Loss: 1.8586\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.9396... Val Loss: 1.8639\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.8977... Val Loss: 1.8547\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.8994... Val Loss: 1.8504\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.9111... Val Loss: 1.8450\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.9339... Val Loss: 1.8402\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.8784... Val Loss: 1.8359\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.8998... Val Loss: 1.8356\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.8817... Val Loss: 1.8342\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.8783... Val Loss: 1.8281\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.8724... Val Loss: 1.8246\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.8846... Val Loss: 1.8257\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.8933... Val Loss: 1.8187\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.8607... Val Loss: 1.8151\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.8993... Val Loss: 1.8091\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.8749... Val Loss: 1.8079\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.8637... Val Loss: 1.8032\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.8896... Val Loss: 1.7995\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.8482... Val Loss: 1.7969\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.8568... Val Loss: 1.7923\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.8379... Val Loss: 1.7894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Step: 1350... Loss: 1.8322... Val Loss: 1.7880\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.8542... Val Loss: 1.7867\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.8207... Val Loss: 1.7834\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.8476... Val Loss: 1.7762\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.8204... Val Loss: 1.7774\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.8540... Val Loss: 1.7709\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.8659... Val Loss: 1.7706\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.8539... Val Loss: 1.7633\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.8218... Val Loss: 1.7615\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.8421... Val Loss: 1.7595\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.7861... Val Loss: 1.7550\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.8125... Val Loss: 1.7527\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.8028... Val Loss: 1.7499\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.8254... Val Loss: 1.7486\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.8084... Val Loss: 1.7447\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.7939... Val Loss: 1.7452\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.7716... Val Loss: 1.7397\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.8355... Val Loss: 1.7393\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.8512... Val Loss: 1.7361\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.8094... Val Loss: 1.7313\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.8259... Val Loss: 1.7318\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.8121... Val Loss: 1.7279\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.7736... Val Loss: 1.7250\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.7470... Val Loss: 1.7251\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.7438... Val Loss: 1.7189\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.7726... Val Loss: 1.7185\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.7621... Val Loss: 1.7156\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.7813... Val Loss: 1.7122\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.7936... Val Loss: 1.7112\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.7629... Val Loss: 1.7093\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.7282... Val Loss: 1.7062\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.7792... Val Loss: 1.7030\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.7718... Val Loss: 1.7021\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.7749... Val Loss: 1.6969\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.7542... Val Loss: 1.6958\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.7536... Val Loss: 1.6930\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.7302... Val Loss: 1.6929\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.7296... Val Loss: 1.6912\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.7710... Val Loss: 1.6867\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.7493... Val Loss: 1.6867\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.7077... Val Loss: 1.6831\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.7468... Val Loss: 1.6795\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.7405... Val Loss: 1.6793\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.7110... Val Loss: 1.6768\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.7139... Val Loss: 1.6715\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.7387... Val Loss: 1.6732\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.7413... Val Loss: 1.6733\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.7385... Val Loss: 1.6663\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.7400... Val Loss: 1.6635\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.6953... Val Loss: 1.6609\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.6711... Val Loss: 1.6606\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.7240... Val Loss: 1.6578\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.7177... Val Loss: 1.6555\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.7284... Val Loss: 1.6563\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.7328... Val Loss: 1.6538\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.7097... Val Loss: 1.6509\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.7379... Val Loss: 1.6503\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.7066... Val Loss: 1.6490\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.6764... Val Loss: 1.6435\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.7324... Val Loss: 1.6433\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.6955... Val Loss: 1.6479\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.7009... Val Loss: 1.6420\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.6792... Val Loss: 1.6399\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.6819... Val Loss: 1.6359\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.6812... Val Loss: 1.6387\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.6619... Val Loss: 1.6326\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.6791... Val Loss: 1.6306\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.7005... Val Loss: 1.6296\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.6714... Val Loss: 1.6274\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.6663... Val Loss: 1.6234\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.6538... Val Loss: 1.6216\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.6811... Val Loss: 1.6217\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.6852... Val Loss: 1.6189\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.6643... Val Loss: 1.6174\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.6615... Val Loss: 1.6201\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.6733... Val Loss: 1.6168\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.6524... Val Loss: 1.6149\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.6801... Val Loss: 1.6110\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.6386... Val Loss: 1.6120\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.6345... Val Loss: 1.6097\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.6588... Val Loss: 1.6067\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.6588... Val Loss: 1.6053\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.6375... Val Loss: 1.6026\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.6266... Val Loss: 1.6008\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.6500... Val Loss: 1.5985\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.6411... Val Loss: 1.5975\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.6079... Val Loss: 1.5960\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.6510... Val Loss: 1.5959\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.6163... Val Loss: 1.5985\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.6340... Val Loss: 1.5918\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.6208... Val Loss: 1.5928\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.6434... Val Loss: 1.5890\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.6453... Val Loss: 1.5908\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.6325... Val Loss: 1.5879\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.6239... Val Loss: 1.5850\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.5947... Val Loss: 1.5833\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.6227... Val Loss: 1.5814\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.6122... Val Loss: 1.5790\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.6216... Val Loss: 1.5791\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.6339... Val Loss: 1.5776\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.6214... Val Loss: 1.5751\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.6369... Val Loss: 1.5758\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.6247... Val Loss: 1.5796\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.6111... Val Loss: 1.5717\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.6057... Val Loss: 1.5703\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.6470... Val Loss: 1.5709\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.6317... Val Loss: 1.5708\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.6008... Val Loss: 1.5691\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.6095... Val Loss: 1.5666\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.6047... Val Loss: 1.5647\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.6133... Val Loss: 1.5648\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.6108... Val Loss: 1.5644\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.6089... Val Loss: 1.5594\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.6039... Val Loss: 1.5602\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.5965... Val Loss: 1.5586\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.6117... Val Loss: 1.5583\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.6078... Val Loss: 1.5591\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.6059... Val Loss: 1.5530\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.6134... Val Loss: 1.5491\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.6112... Val Loss: 1.5549\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.5826... Val Loss: 1.5509\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.5881... Val Loss: 1.5520\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.5878... Val Loss: 1.5473\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.6242... Val Loss: 1.5467\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.5589... Val Loss: 1.5475\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.5886... Val Loss: 1.5462\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.5876... Val Loss: 1.5440\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.5720... Val Loss: 1.5453\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.5805... Val Loss: 1.5427\n",
      "Epoch: 19/20... Step: 2640... Loss: 1.5967... Val Loss: 1.5449\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.6047... Val Loss: 1.5446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20... Step: 2660... Loss: 1.5833... Val Loss: 1.5404\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.6030... Val Loss: 1.5352\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.5876... Val Loss: 1.5418\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.5752... Val Loss: 1.5372\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.5931... Val Loss: 1.5342\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.5597... Val Loss: 1.5339\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.5621... Val Loss: 1.5336\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.5490... Val Loss: 1.5338\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.5538... Val Loss: 1.5310\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.5647... Val Loss: 1.5293\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.5375... Val Loss: 1.5323\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.5746... Val Loss: 1.5257\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.5507... Val Loss: 1.5276\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20\n",
    "\n",
    "train(net,encoded,epochs=n_epochs,batch_size=batch_size,seq_length=seq_length,lr=0.001,print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {\n",
    "    'n_hidden':net.n_hidden,\n",
    "    'n_layers':net.n_layers,\n",
    "    'state_dict':net.state_dict(),\n",
    "    'tokens':net.chars\n",
    "}\n",
    "\n",
    "with open(model_name,'wb') as f:\n",
    "    torch.save(checkpoint,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net,char,h=None,top_k=None):\n",
    "    \n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    x = one_hot_encode(x,len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "    \n",
    "    if(gpu):\n",
    "        inputs = inputs.cuda()\n",
    "        \n",
    "    h = tuple([each.data for each in h])\n",
    "    out,h = net(inputs,h)\n",
    "    \n",
    "    p = F.softmax(out,dim=1).data\n",
    "    \n",
    "    if(gpu):\n",
    "        p = p.cpu()\n",
    "        \n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p,top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "    \n",
    "    p = p.numpy().squeeze()\n",
    "    \n",
    "    char = np.random.choice(top_ch,p=p/p.sum())\n",
    "    \n",
    "    return net.int2char[char],h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aswing him on\n",
      "anyont on the fast of the fartent feening and she wondered to tell a sont. She well\n",
      "see the face of a seeming. The man this\n",
      "saw over her sations of tho high and had nit of the persons and had becone the placess with\n",
      "a\n",
      "somprant to him and his sincessen out the mentor that he would not help all a sooting herself.\n",
      "\n",
      "\"What when have the\n",
      "streist well and\n",
      "that the\n",
      "door, but the done. A countes, toot in the care word of, but they's and have an otners of the misure and so he was\n",
      "suppeced all at it. It's\n",
      "brideling to be and that he went a that there winding this all that so that you stint as a mattings, but you was not\n",
      "an tall in her a monessed and to the same who she seemed his simple, and well, a staye of home.\n",
      "And you have thinking at the conversation at ansine of husbands who was all his mass with a change with all the face, that simple the securation of their son. And as he have a mostense, and still you work how a ponce time in time. He was to her, I conterved at the\n",
      "somp of at through, when the window.\"\n",
      "\n",
      "\"Well...\" Levin said herself at a crince. Alexey Alexandrovitca was a stuble the state that there asked in something highed of the same tomater houre as he had as sating anown to have say to be the stope of his stemped.\n",
      "\n",
      "\"I dase to think\n",
      "as it think was, and the surmet of myshand.\n",
      "Though what would stay anyther, but the course, but\n",
      "it was not, but the done of these\n",
      "more the straiched was to begin all that he sent out on the face.\" Alexey Alexandrovitch was not her head.\n",
      " And\n",
      "he was all of this. All the cornented as the same of she came into\n",
      "his\n",
      "carritic at had as he saw him. The driver to\n",
      "this sending and whol he had been bees than the sack were of interest of his how of his saminity and all\n",
      "had\n",
      "the thought of the chomenes would had all the sole talk, as he was said\n",
      "one\n",
      "weanted of his.\n",
      "\n",
      "Anna she was not a counting\n",
      "house of allought his still. And with the prencouse and wasted her foothed, and he could not how the treated, house to arring her felling of a prince\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"Aswin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
